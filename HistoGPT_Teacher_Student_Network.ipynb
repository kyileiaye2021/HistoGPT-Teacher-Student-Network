{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMEgkNjvrBJsuwO8Xcvm98g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyileiaye2021/HistoGPT-Teacher-Student-Network/blob/main/HistoGPT_Teacher_Student_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup environment"
      ],
      "metadata": {
        "id": "StkPp5VXN48S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install openslide dependencies\n",
        "!sudo apt-get install openslide-tools\n",
        "!sudo apt-get install python-openslide\n",
        "!pip install openslide-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNphB3ACN6aO",
        "outputId": "b9214ab4-72a7-4cc6-b6d9-1d671db6909d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openslide-tools is already the newest version (3.4.1+dfsg-5build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package python-openslide\n",
            "Requirement already satisfied: openslide-python in /usr/local/lib/python3.12/dist-packages (1.4.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from openslide-python) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install flamingo and histogpt\n",
        "!pip install flamingo-pytorch --no-deps\n",
        "!pip install git+https://github.com/marrlab/HistoGPT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia7hnIXeN8Iv",
        "outputId": "534fdb2c-3667-4f0d-c100-c0068f1501fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flamingo-pytorch in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Collecting git+https://github.com/marrlab/HistoGPT.git\n",
            "  Cloning https://github.com/marrlab/HistoGPT.git to /tmp/pip-req-build-spq7nhzw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/marrlab/HistoGPT.git /tmp/pip-req-build-spq7nhzw\n",
            "  Resolved https://github.com/marrlab/HistoGPT.git to commit 35feddc2b5833676e9e8f09ee432b548a2a75e46\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (0.8.1)\n",
            "Requirement already satisfied: einops-exts in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (0.0.4)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (1.106.1)\n",
            "Requirement already satisfied: sacremoses>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (0.1.1)\n",
            "Requirement already satisfied: slideio>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (2.7.2)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers==4.38.2 in /usr/local/lib/python3.12/dist-packages (from histogpt==1.1.2) (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.38.2->histogpt==1.1.2) (4.67.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.14.0->histogpt==1.1.2) (4.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from sacremoses>=0.1.1->histogpt==1.1.2) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from sacremoses>=0.1.1->histogpt==1.1.2) (1.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->histogpt==1.1.2) (3.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.14.0->histogpt==1.1.2) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.14.0->histogpt==1.1.2) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai>=1.14.0->histogpt==1.1.2) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.14.0->histogpt==1.1.2) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2->histogpt==1.1.2) (1.1.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.14.0->histogpt==1.1.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.14.0->histogpt==1.1.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai>=1.14.0->histogpt==1.1.2) (0.4.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->histogpt==1.1.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->histogpt==1.1.2) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2->histogpt==1.1.2) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.38.2->histogpt==1.1.2) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether to use a gpu or cpu\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8BvX7TzN-cj",
        "outputId": "2917d8d2-cef9-4d63-cfe7-477b8e5a6a89"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting the Google Drive"
      ],
      "metadata": {
        "id": "XNxbldYKOCSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JOSUz_pOFAy",
        "outputId": "6b5208bd-defd-499b-cd8a-789527afad07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the HistoGPT Teacher model (original model)"
      ],
      "metadata": {
        "id": "qsGuQ7agOG3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BioGptConfig\n",
        "from histogpt.models import HistoGPTForCausalLM, PerceiverResamplerConfig\n",
        "\n",
        "histogpt_teacher = HistoGPTForCausalLM(BioGptConfig(), PerceiverResamplerConfig())\n",
        "histogpt_teacher = histogpt_teacher.to(device)\n",
        "PATH = '/content/drive/MyDrive/Teacher_Student_Network/Histo GPT/Histogpt_weights/histogpt-1b-6k-pruned.pth' # histogpt weight\n",
        "state_dict = torch.load(PATH, map_location=device)\n",
        "histogpt_teacher.load_state_dict(state_dict, strict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDubaCxqOf63",
        "outputId": "db8cf434-3567-4edd-be7f-8d5edb287e1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking HistoGPT Teacher model parameters"
      ],
      "metadata": {
        "id": "KXsCYrsSO0nx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in histogpt_teacher.named_parameters():\n",
        "    print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrQCYAaEOrOD",
        "outputId": "6b47544a-0356-4f17-87af-070edfba1a21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "histogpt.perceiver_resampler.media_pos\n",
            "histogpt.perceiver_resampler.latents\n",
            "histogpt.perceiver_resampler.linear.weight\n",
            "histogpt.perceiver_resampler.layers.0.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.0.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.0.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.0.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.0.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.0.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.0.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.0.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.0.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.0.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.0.1.3.weight\n",
            "histogpt.perceiver_resampler.layers.1.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.1.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.1.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.1.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.1.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.1.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.1.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.1.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.1.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.1.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.1.1.3.weight\n",
            "histogpt.perceiver_resampler.layers.2.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.2.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.2.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.2.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.2.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.2.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.2.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.2.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.2.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.2.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.2.1.3.weight\n",
            "histogpt.perceiver_resampler.layers.3.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.3.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.3.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.3.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.3.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.3.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.3.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.3.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.3.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.3.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.3.1.3.weight\n",
            "histogpt.perceiver_resampler.layers.4.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.4.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.4.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.4.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.4.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.4.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.4.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.4.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.4.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.4.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.4.1.3.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.5.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.5.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.5.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.5.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.5.1.3.weight\n",
            "histogpt.perceiver_resampler.norm.weight\n",
            "histogpt.perceiver_resampler.norm.bias\n",
            "histogpt.perceiver_exitgate.weight\n",
            "histogpt.embed_tokens.weight\n",
            "histogpt.embed_positions.weight\n",
            "histogpt.layers.0.0.attn_gate\n",
            "histogpt.layers.0.0.ff_gate\n",
            "histogpt.layers.0.0.attn.norm.weight\n",
            "histogpt.layers.0.0.attn.norm.bias\n",
            "histogpt.layers.0.0.attn.to_q.weight\n",
            "histogpt.layers.0.0.attn.to_kv.weight\n",
            "histogpt.layers.0.0.attn.to_out.weight\n",
            "histogpt.layers.0.0.ff.0.weight\n",
            "histogpt.layers.0.0.ff.0.bias\n",
            "histogpt.layers.0.0.ff.1.weight\n",
            "histogpt.layers.0.0.ff.3.weight\n",
            "histogpt.layers.0.1.self_attn.k_proj.weight\n",
            "histogpt.layers.0.1.self_attn.k_proj.bias\n",
            "histogpt.layers.0.1.self_attn.v_proj.weight\n",
            "histogpt.layers.0.1.self_attn.v_proj.bias\n",
            "histogpt.layers.0.1.self_attn.q_proj.weight\n",
            "histogpt.layers.0.1.self_attn.q_proj.bias\n",
            "histogpt.layers.0.1.self_attn.out_proj.weight\n",
            "histogpt.layers.0.1.self_attn.out_proj.bias\n",
            "histogpt.layers.0.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.0.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.0.1.fc1.weight\n",
            "histogpt.layers.0.1.fc1.bias\n",
            "histogpt.layers.0.1.fc2.weight\n",
            "histogpt.layers.0.1.fc2.bias\n",
            "histogpt.layers.0.1.final_layer_norm.weight\n",
            "histogpt.layers.0.1.final_layer_norm.bias\n",
            "histogpt.layers.1.0.attn_gate\n",
            "histogpt.layers.1.0.ff_gate\n",
            "histogpt.layers.1.0.attn.norm.weight\n",
            "histogpt.layers.1.0.attn.norm.bias\n",
            "histogpt.layers.1.0.attn.to_q.weight\n",
            "histogpt.layers.1.0.attn.to_kv.weight\n",
            "histogpt.layers.1.0.attn.to_out.weight\n",
            "histogpt.layers.1.0.ff.0.weight\n",
            "histogpt.layers.1.0.ff.0.bias\n",
            "histogpt.layers.1.0.ff.1.weight\n",
            "histogpt.layers.1.0.ff.3.weight\n",
            "histogpt.layers.1.1.self_attn.k_proj.weight\n",
            "histogpt.layers.1.1.self_attn.k_proj.bias\n",
            "histogpt.layers.1.1.self_attn.v_proj.weight\n",
            "histogpt.layers.1.1.self_attn.v_proj.bias\n",
            "histogpt.layers.1.1.self_attn.q_proj.weight\n",
            "histogpt.layers.1.1.self_attn.q_proj.bias\n",
            "histogpt.layers.1.1.self_attn.out_proj.weight\n",
            "histogpt.layers.1.1.self_attn.out_proj.bias\n",
            "histogpt.layers.1.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.1.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.1.1.fc1.weight\n",
            "histogpt.layers.1.1.fc1.bias\n",
            "histogpt.layers.1.1.fc2.weight\n",
            "histogpt.layers.1.1.fc2.bias\n",
            "histogpt.layers.1.1.final_layer_norm.weight\n",
            "histogpt.layers.1.1.final_layer_norm.bias\n",
            "histogpt.layers.2.0.attn_gate\n",
            "histogpt.layers.2.0.ff_gate\n",
            "histogpt.layers.2.0.attn.norm.weight\n",
            "histogpt.layers.2.0.attn.norm.bias\n",
            "histogpt.layers.2.0.attn.to_q.weight\n",
            "histogpt.layers.2.0.attn.to_kv.weight\n",
            "histogpt.layers.2.0.attn.to_out.weight\n",
            "histogpt.layers.2.0.ff.0.weight\n",
            "histogpt.layers.2.0.ff.0.bias\n",
            "histogpt.layers.2.0.ff.1.weight\n",
            "histogpt.layers.2.0.ff.3.weight\n",
            "histogpt.layers.2.1.self_attn.k_proj.weight\n",
            "histogpt.layers.2.1.self_attn.k_proj.bias\n",
            "histogpt.layers.2.1.self_attn.v_proj.weight\n",
            "histogpt.layers.2.1.self_attn.v_proj.bias\n",
            "histogpt.layers.2.1.self_attn.q_proj.weight\n",
            "histogpt.layers.2.1.self_attn.q_proj.bias\n",
            "histogpt.layers.2.1.self_attn.out_proj.weight\n",
            "histogpt.layers.2.1.self_attn.out_proj.bias\n",
            "histogpt.layers.2.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.2.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.2.1.fc1.weight\n",
            "histogpt.layers.2.1.fc1.bias\n",
            "histogpt.layers.2.1.fc2.weight\n",
            "histogpt.layers.2.1.fc2.bias\n",
            "histogpt.layers.2.1.final_layer_norm.weight\n",
            "histogpt.layers.2.1.final_layer_norm.bias\n",
            "histogpt.layers.3.0.attn_gate\n",
            "histogpt.layers.3.0.ff_gate\n",
            "histogpt.layers.3.0.attn.norm.weight\n",
            "histogpt.layers.3.0.attn.norm.bias\n",
            "histogpt.layers.3.0.attn.to_q.weight\n",
            "histogpt.layers.3.0.attn.to_kv.weight\n",
            "histogpt.layers.3.0.attn.to_out.weight\n",
            "histogpt.layers.3.0.ff.0.weight\n",
            "histogpt.layers.3.0.ff.0.bias\n",
            "histogpt.layers.3.0.ff.1.weight\n",
            "histogpt.layers.3.0.ff.3.weight\n",
            "histogpt.layers.3.1.self_attn.k_proj.weight\n",
            "histogpt.layers.3.1.self_attn.k_proj.bias\n",
            "histogpt.layers.3.1.self_attn.v_proj.weight\n",
            "histogpt.layers.3.1.self_attn.v_proj.bias\n",
            "histogpt.layers.3.1.self_attn.q_proj.weight\n",
            "histogpt.layers.3.1.self_attn.q_proj.bias\n",
            "histogpt.layers.3.1.self_attn.out_proj.weight\n",
            "histogpt.layers.3.1.self_attn.out_proj.bias\n",
            "histogpt.layers.3.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.3.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.3.1.fc1.weight\n",
            "histogpt.layers.3.1.fc1.bias\n",
            "histogpt.layers.3.1.fc2.weight\n",
            "histogpt.layers.3.1.fc2.bias\n",
            "histogpt.layers.3.1.final_layer_norm.weight\n",
            "histogpt.layers.3.1.final_layer_norm.bias\n",
            "histogpt.layers.4.0.attn_gate\n",
            "histogpt.layers.4.0.ff_gate\n",
            "histogpt.layers.4.0.attn.norm.weight\n",
            "histogpt.layers.4.0.attn.norm.bias\n",
            "histogpt.layers.4.0.attn.to_q.weight\n",
            "histogpt.layers.4.0.attn.to_kv.weight\n",
            "histogpt.layers.4.0.attn.to_out.weight\n",
            "histogpt.layers.4.0.ff.0.weight\n",
            "histogpt.layers.4.0.ff.0.bias\n",
            "histogpt.layers.4.0.ff.1.weight\n",
            "histogpt.layers.4.0.ff.3.weight\n",
            "histogpt.layers.4.1.self_attn.k_proj.weight\n",
            "histogpt.layers.4.1.self_attn.k_proj.bias\n",
            "histogpt.layers.4.1.self_attn.v_proj.weight\n",
            "histogpt.layers.4.1.self_attn.v_proj.bias\n",
            "histogpt.layers.4.1.self_attn.q_proj.weight\n",
            "histogpt.layers.4.1.self_attn.q_proj.bias\n",
            "histogpt.layers.4.1.self_attn.out_proj.weight\n",
            "histogpt.layers.4.1.self_attn.out_proj.bias\n",
            "histogpt.layers.4.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.4.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.4.1.fc1.weight\n",
            "histogpt.layers.4.1.fc1.bias\n",
            "histogpt.layers.4.1.fc2.weight\n",
            "histogpt.layers.4.1.fc2.bias\n",
            "histogpt.layers.4.1.final_layer_norm.weight\n",
            "histogpt.layers.4.1.final_layer_norm.bias\n",
            "histogpt.layers.5.0.attn_gate\n",
            "histogpt.layers.5.0.ff_gate\n",
            "histogpt.layers.5.0.attn.norm.weight\n",
            "histogpt.layers.5.0.attn.norm.bias\n",
            "histogpt.layers.5.0.attn.to_q.weight\n",
            "histogpt.layers.5.0.attn.to_kv.weight\n",
            "histogpt.layers.5.0.attn.to_out.weight\n",
            "histogpt.layers.5.0.ff.0.weight\n",
            "histogpt.layers.5.0.ff.0.bias\n",
            "histogpt.layers.5.0.ff.1.weight\n",
            "histogpt.layers.5.0.ff.3.weight\n",
            "histogpt.layers.5.1.self_attn.k_proj.weight\n",
            "histogpt.layers.5.1.self_attn.k_proj.bias\n",
            "histogpt.layers.5.1.self_attn.v_proj.weight\n",
            "histogpt.layers.5.1.self_attn.v_proj.bias\n",
            "histogpt.layers.5.1.self_attn.q_proj.weight\n",
            "histogpt.layers.5.1.self_attn.q_proj.bias\n",
            "histogpt.layers.5.1.self_attn.out_proj.weight\n",
            "histogpt.layers.5.1.self_attn.out_proj.bias\n",
            "histogpt.layers.5.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.5.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.5.1.fc1.weight\n",
            "histogpt.layers.5.1.fc1.bias\n",
            "histogpt.layers.5.1.fc2.weight\n",
            "histogpt.layers.5.1.fc2.bias\n",
            "histogpt.layers.5.1.final_layer_norm.weight\n",
            "histogpt.layers.5.1.final_layer_norm.bias\n",
            "histogpt.layers.6.0.attn_gate\n",
            "histogpt.layers.6.0.ff_gate\n",
            "histogpt.layers.6.0.attn.norm.weight\n",
            "histogpt.layers.6.0.attn.norm.bias\n",
            "histogpt.layers.6.0.attn.to_q.weight\n",
            "histogpt.layers.6.0.attn.to_kv.weight\n",
            "histogpt.layers.6.0.attn.to_out.weight\n",
            "histogpt.layers.6.0.ff.0.weight\n",
            "histogpt.layers.6.0.ff.0.bias\n",
            "histogpt.layers.6.0.ff.1.weight\n",
            "histogpt.layers.6.0.ff.3.weight\n",
            "histogpt.layers.6.1.self_attn.k_proj.weight\n",
            "histogpt.layers.6.1.self_attn.k_proj.bias\n",
            "histogpt.layers.6.1.self_attn.v_proj.weight\n",
            "histogpt.layers.6.1.self_attn.v_proj.bias\n",
            "histogpt.layers.6.1.self_attn.q_proj.weight\n",
            "histogpt.layers.6.1.self_attn.q_proj.bias\n",
            "histogpt.layers.6.1.self_attn.out_proj.weight\n",
            "histogpt.layers.6.1.self_attn.out_proj.bias\n",
            "histogpt.layers.6.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.6.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.6.1.fc1.weight\n",
            "histogpt.layers.6.1.fc1.bias\n",
            "histogpt.layers.6.1.fc2.weight\n",
            "histogpt.layers.6.1.fc2.bias\n",
            "histogpt.layers.6.1.final_layer_norm.weight\n",
            "histogpt.layers.6.1.final_layer_norm.bias\n",
            "histogpt.layers.7.0.attn_gate\n",
            "histogpt.layers.7.0.ff_gate\n",
            "histogpt.layers.7.0.attn.norm.weight\n",
            "histogpt.layers.7.0.attn.norm.bias\n",
            "histogpt.layers.7.0.attn.to_q.weight\n",
            "histogpt.layers.7.0.attn.to_kv.weight\n",
            "histogpt.layers.7.0.attn.to_out.weight\n",
            "histogpt.layers.7.0.ff.0.weight\n",
            "histogpt.layers.7.0.ff.0.bias\n",
            "histogpt.layers.7.0.ff.1.weight\n",
            "histogpt.layers.7.0.ff.3.weight\n",
            "histogpt.layers.7.1.self_attn.k_proj.weight\n",
            "histogpt.layers.7.1.self_attn.k_proj.bias\n",
            "histogpt.layers.7.1.self_attn.v_proj.weight\n",
            "histogpt.layers.7.1.self_attn.v_proj.bias\n",
            "histogpt.layers.7.1.self_attn.q_proj.weight\n",
            "histogpt.layers.7.1.self_attn.q_proj.bias\n",
            "histogpt.layers.7.1.self_attn.out_proj.weight\n",
            "histogpt.layers.7.1.self_attn.out_proj.bias\n",
            "histogpt.layers.7.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.7.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.7.1.fc1.weight\n",
            "histogpt.layers.7.1.fc1.bias\n",
            "histogpt.layers.7.1.fc2.weight\n",
            "histogpt.layers.7.1.fc2.bias\n",
            "histogpt.layers.7.1.final_layer_norm.weight\n",
            "histogpt.layers.7.1.final_layer_norm.bias\n",
            "histogpt.layers.8.0.attn_gate\n",
            "histogpt.layers.8.0.ff_gate\n",
            "histogpt.layers.8.0.attn.norm.weight\n",
            "histogpt.layers.8.0.attn.norm.bias\n",
            "histogpt.layers.8.0.attn.to_q.weight\n",
            "histogpt.layers.8.0.attn.to_kv.weight\n",
            "histogpt.layers.8.0.attn.to_out.weight\n",
            "histogpt.layers.8.0.ff.0.weight\n",
            "histogpt.layers.8.0.ff.0.bias\n",
            "histogpt.layers.8.0.ff.1.weight\n",
            "histogpt.layers.8.0.ff.3.weight\n",
            "histogpt.layers.8.1.self_attn.k_proj.weight\n",
            "histogpt.layers.8.1.self_attn.k_proj.bias\n",
            "histogpt.layers.8.1.self_attn.v_proj.weight\n",
            "histogpt.layers.8.1.self_attn.v_proj.bias\n",
            "histogpt.layers.8.1.self_attn.q_proj.weight\n",
            "histogpt.layers.8.1.self_attn.q_proj.bias\n",
            "histogpt.layers.8.1.self_attn.out_proj.weight\n",
            "histogpt.layers.8.1.self_attn.out_proj.bias\n",
            "histogpt.layers.8.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.8.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.8.1.fc1.weight\n",
            "histogpt.layers.8.1.fc1.bias\n",
            "histogpt.layers.8.1.fc2.weight\n",
            "histogpt.layers.8.1.fc2.bias\n",
            "histogpt.layers.8.1.final_layer_norm.weight\n",
            "histogpt.layers.8.1.final_layer_norm.bias\n",
            "histogpt.layers.9.0.attn_gate\n",
            "histogpt.layers.9.0.ff_gate\n",
            "histogpt.layers.9.0.attn.norm.weight\n",
            "histogpt.layers.9.0.attn.norm.bias\n",
            "histogpt.layers.9.0.attn.to_q.weight\n",
            "histogpt.layers.9.0.attn.to_kv.weight\n",
            "histogpt.layers.9.0.attn.to_out.weight\n",
            "histogpt.layers.9.0.ff.0.weight\n",
            "histogpt.layers.9.0.ff.0.bias\n",
            "histogpt.layers.9.0.ff.1.weight\n",
            "histogpt.layers.9.0.ff.3.weight\n",
            "histogpt.layers.9.1.self_attn.k_proj.weight\n",
            "histogpt.layers.9.1.self_attn.k_proj.bias\n",
            "histogpt.layers.9.1.self_attn.v_proj.weight\n",
            "histogpt.layers.9.1.self_attn.v_proj.bias\n",
            "histogpt.layers.9.1.self_attn.q_proj.weight\n",
            "histogpt.layers.9.1.self_attn.q_proj.bias\n",
            "histogpt.layers.9.1.self_attn.out_proj.weight\n",
            "histogpt.layers.9.1.self_attn.out_proj.bias\n",
            "histogpt.layers.9.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.9.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.9.1.fc1.weight\n",
            "histogpt.layers.9.1.fc1.bias\n",
            "histogpt.layers.9.1.fc2.weight\n",
            "histogpt.layers.9.1.fc2.bias\n",
            "histogpt.layers.9.1.final_layer_norm.weight\n",
            "histogpt.layers.9.1.final_layer_norm.bias\n",
            "histogpt.layers.10.0.attn_gate\n",
            "histogpt.layers.10.0.ff_gate\n",
            "histogpt.layers.10.0.attn.norm.weight\n",
            "histogpt.layers.10.0.attn.norm.bias\n",
            "histogpt.layers.10.0.attn.to_q.weight\n",
            "histogpt.layers.10.0.attn.to_kv.weight\n",
            "histogpt.layers.10.0.attn.to_out.weight\n",
            "histogpt.layers.10.0.ff.0.weight\n",
            "histogpt.layers.10.0.ff.0.bias\n",
            "histogpt.layers.10.0.ff.1.weight\n",
            "histogpt.layers.10.0.ff.3.weight\n",
            "histogpt.layers.10.1.self_attn.k_proj.weight\n",
            "histogpt.layers.10.1.self_attn.k_proj.bias\n",
            "histogpt.layers.10.1.self_attn.v_proj.weight\n",
            "histogpt.layers.10.1.self_attn.v_proj.bias\n",
            "histogpt.layers.10.1.self_attn.q_proj.weight\n",
            "histogpt.layers.10.1.self_attn.q_proj.bias\n",
            "histogpt.layers.10.1.self_attn.out_proj.weight\n",
            "histogpt.layers.10.1.self_attn.out_proj.bias\n",
            "histogpt.layers.10.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.10.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.10.1.fc1.weight\n",
            "histogpt.layers.10.1.fc1.bias\n",
            "histogpt.layers.10.1.fc2.weight\n",
            "histogpt.layers.10.1.fc2.bias\n",
            "histogpt.layers.10.1.final_layer_norm.weight\n",
            "histogpt.layers.10.1.final_layer_norm.bias\n",
            "histogpt.layers.11.0.attn_gate\n",
            "histogpt.layers.11.0.ff_gate\n",
            "histogpt.layers.11.0.attn.norm.weight\n",
            "histogpt.layers.11.0.attn.norm.bias\n",
            "histogpt.layers.11.0.attn.to_q.weight\n",
            "histogpt.layers.11.0.attn.to_kv.weight\n",
            "histogpt.layers.11.0.attn.to_out.weight\n",
            "histogpt.layers.11.0.ff.0.weight\n",
            "histogpt.layers.11.0.ff.0.bias\n",
            "histogpt.layers.11.0.ff.1.weight\n",
            "histogpt.layers.11.0.ff.3.weight\n",
            "histogpt.layers.11.1.self_attn.k_proj.weight\n",
            "histogpt.layers.11.1.self_attn.k_proj.bias\n",
            "histogpt.layers.11.1.self_attn.v_proj.weight\n",
            "histogpt.layers.11.1.self_attn.v_proj.bias\n",
            "histogpt.layers.11.1.self_attn.q_proj.weight\n",
            "histogpt.layers.11.1.self_attn.q_proj.bias\n",
            "histogpt.layers.11.1.self_attn.out_proj.weight\n",
            "histogpt.layers.11.1.self_attn.out_proj.bias\n",
            "histogpt.layers.11.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.11.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.11.1.fc1.weight\n",
            "histogpt.layers.11.1.fc1.bias\n",
            "histogpt.layers.11.1.fc2.weight\n",
            "histogpt.layers.11.1.fc2.bias\n",
            "histogpt.layers.11.1.final_layer_norm.weight\n",
            "histogpt.layers.11.1.final_layer_norm.bias\n",
            "histogpt.layers.12.0.attn_gate\n",
            "histogpt.layers.12.0.ff_gate\n",
            "histogpt.layers.12.0.attn.norm.weight\n",
            "histogpt.layers.12.0.attn.norm.bias\n",
            "histogpt.layers.12.0.attn.to_q.weight\n",
            "histogpt.layers.12.0.attn.to_kv.weight\n",
            "histogpt.layers.12.0.attn.to_out.weight\n",
            "histogpt.layers.12.0.ff.0.weight\n",
            "histogpt.layers.12.0.ff.0.bias\n",
            "histogpt.layers.12.0.ff.1.weight\n",
            "histogpt.layers.12.0.ff.3.weight\n",
            "histogpt.layers.12.1.self_attn.k_proj.weight\n",
            "histogpt.layers.12.1.self_attn.k_proj.bias\n",
            "histogpt.layers.12.1.self_attn.v_proj.weight\n",
            "histogpt.layers.12.1.self_attn.v_proj.bias\n",
            "histogpt.layers.12.1.self_attn.q_proj.weight\n",
            "histogpt.layers.12.1.self_attn.q_proj.bias\n",
            "histogpt.layers.12.1.self_attn.out_proj.weight\n",
            "histogpt.layers.12.1.self_attn.out_proj.bias\n",
            "histogpt.layers.12.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.12.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.12.1.fc1.weight\n",
            "histogpt.layers.12.1.fc1.bias\n",
            "histogpt.layers.12.1.fc2.weight\n",
            "histogpt.layers.12.1.fc2.bias\n",
            "histogpt.layers.12.1.final_layer_norm.weight\n",
            "histogpt.layers.12.1.final_layer_norm.bias\n",
            "histogpt.layers.13.0.attn_gate\n",
            "histogpt.layers.13.0.ff_gate\n",
            "histogpt.layers.13.0.attn.norm.weight\n",
            "histogpt.layers.13.0.attn.norm.bias\n",
            "histogpt.layers.13.0.attn.to_q.weight\n",
            "histogpt.layers.13.0.attn.to_kv.weight\n",
            "histogpt.layers.13.0.attn.to_out.weight\n",
            "histogpt.layers.13.0.ff.0.weight\n",
            "histogpt.layers.13.0.ff.0.bias\n",
            "histogpt.layers.13.0.ff.1.weight\n",
            "histogpt.layers.13.0.ff.3.weight\n",
            "histogpt.layers.13.1.self_attn.k_proj.weight\n",
            "histogpt.layers.13.1.self_attn.k_proj.bias\n",
            "histogpt.layers.13.1.self_attn.v_proj.weight\n",
            "histogpt.layers.13.1.self_attn.v_proj.bias\n",
            "histogpt.layers.13.1.self_attn.q_proj.weight\n",
            "histogpt.layers.13.1.self_attn.q_proj.bias\n",
            "histogpt.layers.13.1.self_attn.out_proj.weight\n",
            "histogpt.layers.13.1.self_attn.out_proj.bias\n",
            "histogpt.layers.13.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.13.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.13.1.fc1.weight\n",
            "histogpt.layers.13.1.fc1.bias\n",
            "histogpt.layers.13.1.fc2.weight\n",
            "histogpt.layers.13.1.fc2.bias\n",
            "histogpt.layers.13.1.final_layer_norm.weight\n",
            "histogpt.layers.13.1.final_layer_norm.bias\n",
            "histogpt.layers.14.0.attn_gate\n",
            "histogpt.layers.14.0.ff_gate\n",
            "histogpt.layers.14.0.attn.norm.weight\n",
            "histogpt.layers.14.0.attn.norm.bias\n",
            "histogpt.layers.14.0.attn.to_q.weight\n",
            "histogpt.layers.14.0.attn.to_kv.weight\n",
            "histogpt.layers.14.0.attn.to_out.weight\n",
            "histogpt.layers.14.0.ff.0.weight\n",
            "histogpt.layers.14.0.ff.0.bias\n",
            "histogpt.layers.14.0.ff.1.weight\n",
            "histogpt.layers.14.0.ff.3.weight\n",
            "histogpt.layers.14.1.self_attn.k_proj.weight\n",
            "histogpt.layers.14.1.self_attn.k_proj.bias\n",
            "histogpt.layers.14.1.self_attn.v_proj.weight\n",
            "histogpt.layers.14.1.self_attn.v_proj.bias\n",
            "histogpt.layers.14.1.self_attn.q_proj.weight\n",
            "histogpt.layers.14.1.self_attn.q_proj.bias\n",
            "histogpt.layers.14.1.self_attn.out_proj.weight\n",
            "histogpt.layers.14.1.self_attn.out_proj.bias\n",
            "histogpt.layers.14.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.14.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.14.1.fc1.weight\n",
            "histogpt.layers.14.1.fc1.bias\n",
            "histogpt.layers.14.1.fc2.weight\n",
            "histogpt.layers.14.1.fc2.bias\n",
            "histogpt.layers.14.1.final_layer_norm.weight\n",
            "histogpt.layers.14.1.final_layer_norm.bias\n",
            "histogpt.layers.15.0.attn_gate\n",
            "histogpt.layers.15.0.ff_gate\n",
            "histogpt.layers.15.0.attn.norm.weight\n",
            "histogpt.layers.15.0.attn.norm.bias\n",
            "histogpt.layers.15.0.attn.to_q.weight\n",
            "histogpt.layers.15.0.attn.to_kv.weight\n",
            "histogpt.layers.15.0.attn.to_out.weight\n",
            "histogpt.layers.15.0.ff.0.weight\n",
            "histogpt.layers.15.0.ff.0.bias\n",
            "histogpt.layers.15.0.ff.1.weight\n",
            "histogpt.layers.15.0.ff.3.weight\n",
            "histogpt.layers.15.1.self_attn.k_proj.weight\n",
            "histogpt.layers.15.1.self_attn.k_proj.bias\n",
            "histogpt.layers.15.1.self_attn.v_proj.weight\n",
            "histogpt.layers.15.1.self_attn.v_proj.bias\n",
            "histogpt.layers.15.1.self_attn.q_proj.weight\n",
            "histogpt.layers.15.1.self_attn.q_proj.bias\n",
            "histogpt.layers.15.1.self_attn.out_proj.weight\n",
            "histogpt.layers.15.1.self_attn.out_proj.bias\n",
            "histogpt.layers.15.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.15.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.15.1.fc1.weight\n",
            "histogpt.layers.15.1.fc1.bias\n",
            "histogpt.layers.15.1.fc2.weight\n",
            "histogpt.layers.15.1.fc2.bias\n",
            "histogpt.layers.15.1.final_layer_norm.weight\n",
            "histogpt.layers.15.1.final_layer_norm.bias\n",
            "histogpt.layers.16.0.attn_gate\n",
            "histogpt.layers.16.0.ff_gate\n",
            "histogpt.layers.16.0.attn.norm.weight\n",
            "histogpt.layers.16.0.attn.norm.bias\n",
            "histogpt.layers.16.0.attn.to_q.weight\n",
            "histogpt.layers.16.0.attn.to_kv.weight\n",
            "histogpt.layers.16.0.attn.to_out.weight\n",
            "histogpt.layers.16.0.ff.0.weight\n",
            "histogpt.layers.16.0.ff.0.bias\n",
            "histogpt.layers.16.0.ff.1.weight\n",
            "histogpt.layers.16.0.ff.3.weight\n",
            "histogpt.layers.16.1.self_attn.k_proj.weight\n",
            "histogpt.layers.16.1.self_attn.k_proj.bias\n",
            "histogpt.layers.16.1.self_attn.v_proj.weight\n",
            "histogpt.layers.16.1.self_attn.v_proj.bias\n",
            "histogpt.layers.16.1.self_attn.q_proj.weight\n",
            "histogpt.layers.16.1.self_attn.q_proj.bias\n",
            "histogpt.layers.16.1.self_attn.out_proj.weight\n",
            "histogpt.layers.16.1.self_attn.out_proj.bias\n",
            "histogpt.layers.16.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.16.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.16.1.fc1.weight\n",
            "histogpt.layers.16.1.fc1.bias\n",
            "histogpt.layers.16.1.fc2.weight\n",
            "histogpt.layers.16.1.fc2.bias\n",
            "histogpt.layers.16.1.final_layer_norm.weight\n",
            "histogpt.layers.16.1.final_layer_norm.bias\n",
            "histogpt.layers.17.0.attn_gate\n",
            "histogpt.layers.17.0.ff_gate\n",
            "histogpt.layers.17.0.attn.norm.weight\n",
            "histogpt.layers.17.0.attn.norm.bias\n",
            "histogpt.layers.17.0.attn.to_q.weight\n",
            "histogpt.layers.17.0.attn.to_kv.weight\n",
            "histogpt.layers.17.0.attn.to_out.weight\n",
            "histogpt.layers.17.0.ff.0.weight\n",
            "histogpt.layers.17.0.ff.0.bias\n",
            "histogpt.layers.17.0.ff.1.weight\n",
            "histogpt.layers.17.0.ff.3.weight\n",
            "histogpt.layers.17.1.self_attn.k_proj.weight\n",
            "histogpt.layers.17.1.self_attn.k_proj.bias\n",
            "histogpt.layers.17.1.self_attn.v_proj.weight\n",
            "histogpt.layers.17.1.self_attn.v_proj.bias\n",
            "histogpt.layers.17.1.self_attn.q_proj.weight\n",
            "histogpt.layers.17.1.self_attn.q_proj.bias\n",
            "histogpt.layers.17.1.self_attn.out_proj.weight\n",
            "histogpt.layers.17.1.self_attn.out_proj.bias\n",
            "histogpt.layers.17.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.17.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.17.1.fc1.weight\n",
            "histogpt.layers.17.1.fc1.bias\n",
            "histogpt.layers.17.1.fc2.weight\n",
            "histogpt.layers.17.1.fc2.bias\n",
            "histogpt.layers.17.1.final_layer_norm.weight\n",
            "histogpt.layers.17.1.final_layer_norm.bias\n",
            "histogpt.layers.18.0.attn_gate\n",
            "histogpt.layers.18.0.ff_gate\n",
            "histogpt.layers.18.0.attn.norm.weight\n",
            "histogpt.layers.18.0.attn.norm.bias\n",
            "histogpt.layers.18.0.attn.to_q.weight\n",
            "histogpt.layers.18.0.attn.to_kv.weight\n",
            "histogpt.layers.18.0.attn.to_out.weight\n",
            "histogpt.layers.18.0.ff.0.weight\n",
            "histogpt.layers.18.0.ff.0.bias\n",
            "histogpt.layers.18.0.ff.1.weight\n",
            "histogpt.layers.18.0.ff.3.weight\n",
            "histogpt.layers.18.1.self_attn.k_proj.weight\n",
            "histogpt.layers.18.1.self_attn.k_proj.bias\n",
            "histogpt.layers.18.1.self_attn.v_proj.weight\n",
            "histogpt.layers.18.1.self_attn.v_proj.bias\n",
            "histogpt.layers.18.1.self_attn.q_proj.weight\n",
            "histogpt.layers.18.1.self_attn.q_proj.bias\n",
            "histogpt.layers.18.1.self_attn.out_proj.weight\n",
            "histogpt.layers.18.1.self_attn.out_proj.bias\n",
            "histogpt.layers.18.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.18.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.18.1.fc1.weight\n",
            "histogpt.layers.18.1.fc1.bias\n",
            "histogpt.layers.18.1.fc2.weight\n",
            "histogpt.layers.18.1.fc2.bias\n",
            "histogpt.layers.18.1.final_layer_norm.weight\n",
            "histogpt.layers.18.1.final_layer_norm.bias\n",
            "histogpt.layers.19.0.attn_gate\n",
            "histogpt.layers.19.0.ff_gate\n",
            "histogpt.layers.19.0.attn.norm.weight\n",
            "histogpt.layers.19.0.attn.norm.bias\n",
            "histogpt.layers.19.0.attn.to_q.weight\n",
            "histogpt.layers.19.0.attn.to_kv.weight\n",
            "histogpt.layers.19.0.attn.to_out.weight\n",
            "histogpt.layers.19.0.ff.0.weight\n",
            "histogpt.layers.19.0.ff.0.bias\n",
            "histogpt.layers.19.0.ff.1.weight\n",
            "histogpt.layers.19.0.ff.3.weight\n",
            "histogpt.layers.19.1.self_attn.k_proj.weight\n",
            "histogpt.layers.19.1.self_attn.k_proj.bias\n",
            "histogpt.layers.19.1.self_attn.v_proj.weight\n",
            "histogpt.layers.19.1.self_attn.v_proj.bias\n",
            "histogpt.layers.19.1.self_attn.q_proj.weight\n",
            "histogpt.layers.19.1.self_attn.q_proj.bias\n",
            "histogpt.layers.19.1.self_attn.out_proj.weight\n",
            "histogpt.layers.19.1.self_attn.out_proj.bias\n",
            "histogpt.layers.19.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.19.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.19.1.fc1.weight\n",
            "histogpt.layers.19.1.fc1.bias\n",
            "histogpt.layers.19.1.fc2.weight\n",
            "histogpt.layers.19.1.fc2.bias\n",
            "histogpt.layers.19.1.final_layer_norm.weight\n",
            "histogpt.layers.19.1.final_layer_norm.bias\n",
            "histogpt.layers.20.0.attn_gate\n",
            "histogpt.layers.20.0.ff_gate\n",
            "histogpt.layers.20.0.attn.norm.weight\n",
            "histogpt.layers.20.0.attn.norm.bias\n",
            "histogpt.layers.20.0.attn.to_q.weight\n",
            "histogpt.layers.20.0.attn.to_kv.weight\n",
            "histogpt.layers.20.0.attn.to_out.weight\n",
            "histogpt.layers.20.0.ff.0.weight\n",
            "histogpt.layers.20.0.ff.0.bias\n",
            "histogpt.layers.20.0.ff.1.weight\n",
            "histogpt.layers.20.0.ff.3.weight\n",
            "histogpt.layers.20.1.self_attn.k_proj.weight\n",
            "histogpt.layers.20.1.self_attn.k_proj.bias\n",
            "histogpt.layers.20.1.self_attn.v_proj.weight\n",
            "histogpt.layers.20.1.self_attn.v_proj.bias\n",
            "histogpt.layers.20.1.self_attn.q_proj.weight\n",
            "histogpt.layers.20.1.self_attn.q_proj.bias\n",
            "histogpt.layers.20.1.self_attn.out_proj.weight\n",
            "histogpt.layers.20.1.self_attn.out_proj.bias\n",
            "histogpt.layers.20.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.20.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.20.1.fc1.weight\n",
            "histogpt.layers.20.1.fc1.bias\n",
            "histogpt.layers.20.1.fc2.weight\n",
            "histogpt.layers.20.1.fc2.bias\n",
            "histogpt.layers.20.1.final_layer_norm.weight\n",
            "histogpt.layers.20.1.final_layer_norm.bias\n",
            "histogpt.layers.21.0.attn_gate\n",
            "histogpt.layers.21.0.ff_gate\n",
            "histogpt.layers.21.0.attn.norm.weight\n",
            "histogpt.layers.21.0.attn.norm.bias\n",
            "histogpt.layers.21.0.attn.to_q.weight\n",
            "histogpt.layers.21.0.attn.to_kv.weight\n",
            "histogpt.layers.21.0.attn.to_out.weight\n",
            "histogpt.layers.21.0.ff.0.weight\n",
            "histogpt.layers.21.0.ff.0.bias\n",
            "histogpt.layers.21.0.ff.1.weight\n",
            "histogpt.layers.21.0.ff.3.weight\n",
            "histogpt.layers.21.1.self_attn.k_proj.weight\n",
            "histogpt.layers.21.1.self_attn.k_proj.bias\n",
            "histogpt.layers.21.1.self_attn.v_proj.weight\n",
            "histogpt.layers.21.1.self_attn.v_proj.bias\n",
            "histogpt.layers.21.1.self_attn.q_proj.weight\n",
            "histogpt.layers.21.1.self_attn.q_proj.bias\n",
            "histogpt.layers.21.1.self_attn.out_proj.weight\n",
            "histogpt.layers.21.1.self_attn.out_proj.bias\n",
            "histogpt.layers.21.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.21.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.21.1.fc1.weight\n",
            "histogpt.layers.21.1.fc1.bias\n",
            "histogpt.layers.21.1.fc2.weight\n",
            "histogpt.layers.21.1.fc2.bias\n",
            "histogpt.layers.21.1.final_layer_norm.weight\n",
            "histogpt.layers.21.1.final_layer_norm.bias\n",
            "histogpt.layers.22.0.attn_gate\n",
            "histogpt.layers.22.0.ff_gate\n",
            "histogpt.layers.22.0.attn.norm.weight\n",
            "histogpt.layers.22.0.attn.norm.bias\n",
            "histogpt.layers.22.0.attn.to_q.weight\n",
            "histogpt.layers.22.0.attn.to_kv.weight\n",
            "histogpt.layers.22.0.attn.to_out.weight\n",
            "histogpt.layers.22.0.ff.0.weight\n",
            "histogpt.layers.22.0.ff.0.bias\n",
            "histogpt.layers.22.0.ff.1.weight\n",
            "histogpt.layers.22.0.ff.3.weight\n",
            "histogpt.layers.22.1.self_attn.k_proj.weight\n",
            "histogpt.layers.22.1.self_attn.k_proj.bias\n",
            "histogpt.layers.22.1.self_attn.v_proj.weight\n",
            "histogpt.layers.22.1.self_attn.v_proj.bias\n",
            "histogpt.layers.22.1.self_attn.q_proj.weight\n",
            "histogpt.layers.22.1.self_attn.q_proj.bias\n",
            "histogpt.layers.22.1.self_attn.out_proj.weight\n",
            "histogpt.layers.22.1.self_attn.out_proj.bias\n",
            "histogpt.layers.22.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.22.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.22.1.fc1.weight\n",
            "histogpt.layers.22.1.fc1.bias\n",
            "histogpt.layers.22.1.fc2.weight\n",
            "histogpt.layers.22.1.fc2.bias\n",
            "histogpt.layers.22.1.final_layer_norm.weight\n",
            "histogpt.layers.22.1.final_layer_norm.bias\n",
            "histogpt.layers.23.0.attn_gate\n",
            "histogpt.layers.23.0.ff_gate\n",
            "histogpt.layers.23.0.attn.norm.weight\n",
            "histogpt.layers.23.0.attn.norm.bias\n",
            "histogpt.layers.23.0.attn.to_q.weight\n",
            "histogpt.layers.23.0.attn.to_kv.weight\n",
            "histogpt.layers.23.0.attn.to_out.weight\n",
            "histogpt.layers.23.0.ff.0.weight\n",
            "histogpt.layers.23.0.ff.0.bias\n",
            "histogpt.layers.23.0.ff.1.weight\n",
            "histogpt.layers.23.0.ff.3.weight\n",
            "histogpt.layers.23.1.self_attn.k_proj.weight\n",
            "histogpt.layers.23.1.self_attn.k_proj.bias\n",
            "histogpt.layers.23.1.self_attn.v_proj.weight\n",
            "histogpt.layers.23.1.self_attn.v_proj.bias\n",
            "histogpt.layers.23.1.self_attn.q_proj.weight\n",
            "histogpt.layers.23.1.self_attn.q_proj.bias\n",
            "histogpt.layers.23.1.self_attn.out_proj.weight\n",
            "histogpt.layers.23.1.self_attn.out_proj.bias\n",
            "histogpt.layers.23.1.self_attn_layer_norm.weight\n",
            "histogpt.layers.23.1.self_attn_layer_norm.bias\n",
            "histogpt.layers.23.1.fc1.weight\n",
            "histogpt.layers.23.1.fc1.bias\n",
            "histogpt.layers.23.1.fc2.weight\n",
            "histogpt.layers.23.1.fc2.bias\n",
            "histogpt.layers.23.1.final_layer_norm.weight\n",
            "histogpt.layers.23.1.final_layer_norm.bias\n",
            "histogpt.layer_norm.weight\n",
            "histogpt.layer_norm.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Freezing All Trainable Parameters in HistoGPT Teacher Model"
      ],
      "metadata": {
        "id": "XrZMXdDuYa0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing all parameters\n",
        "for param in histogpt_teacher.parameters():\n",
        "    param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "v9dqjb84QJOy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if all parameters are freezed\n",
        "for name, param in histogpt_teacher.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name)"
      ],
      "metadata": {
        "id": "tDGwHEpddGb1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading HistoGPT Student Model"
      ],
      "metadata": {
        "id": "rs7Iv3uil3p8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BioGptConfig\n",
        "from histogpt.models import HistoGPTForCausalLM, PerceiverResamplerConfig\n",
        "\n",
        "histogpt_student = HistoGPTForCausalLM(BioGptConfig(), PerceiverResamplerConfig())\n",
        "histogpt_student = histogpt_student.to(device)\n",
        "PATH = '/content/drive/MyDrive/Teacher_Student_Network/Histo GPT/Histogpt_weights/histogpt-1b-6k-pruned.pth' # histogpt weight\n",
        "state_dict = torch.load(PATH, map_location=device)\n",
        "histogpt_student.load_state_dict(state_dict, strict=True)"
      ],
      "metadata": {
        "id": "_N-E46F7gm9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496c3af2-94f0-48a2-c65d-aa75f49baa6c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unfreezing Certain Parameters in HistoGPT Student Model"
      ],
      "metadata": {
        "id": "RPxZoAVummMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing all parameters first\n",
        "for name, param in histogpt_student.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreezing the last layer of perceiver sampler (slide level encoder)\n",
        "for name, param in histogpt_student.named_parameters():\n",
        "  if 'perceiver_resampler.layers.5' in str(name):\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Unfreezing the exist layer of perceiver sampler\n",
        "for name, param in histogpt_student.named_parameters():\n",
        "  if 'histogpt.perceiver_exitgate.weight' in name:\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "gqw-ZY9kmy9e"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if all parameters are freezed\n",
        "for name, param in histogpt_student.named_parameters():\n",
        "  if param.requires_grad:\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVoEFyweqgFH",
        "outputId": "434fba2a-9618-4259-e0b6-e2949c839415"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "histogpt.perceiver_resampler.layers.5.0.norm_media.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_media.bias\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_latents.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.norm_latents.bias\n",
            "histogpt.perceiver_resampler.layers.5.0.to_q.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.to_kv.weight\n",
            "histogpt.perceiver_resampler.layers.5.0.to_out.weight\n",
            "histogpt.perceiver_resampler.layers.5.1.0.weight\n",
            "histogpt.perceiver_resampler.layers.5.1.0.bias\n",
            "histogpt.perceiver_resampler.layers.5.1.1.weight\n",
            "histogpt.perceiver_resampler.layers.5.1.3.weight\n",
            "histogpt.perceiver_exitgate.weight\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Class"
      ],
      "metadata": {
        "id": "hQuN7FxcjsLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma-ilnvOrxqx",
        "outputId": "a7f13be0-952b-427b-fcf8-4dbf98dca41a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image\n",
        "from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n",
        "\n",
        "class AugmentedImageDataset(Dataset):\n",
        "  def __init__(self, oct_dir, he_dir):\n",
        "    self.oct_dir = oct_dir\n",
        "    self.he_dir = he_dir\n",
        "\n",
        "    self.paired_files = []\n",
        "    he_files = os.listdir(he_dir)\n",
        "    he_files_lower = {he_file.lower(): he_file for he_file in he_files}\n",
        "    for oct_file in os.listdir(oct_dir):\n",
        "      oct_file_lower = oct_file.lower()\n",
        "\n",
        "      # replace '_real_a' with '_real_b' to find the matching files\n",
        "      if '_real_a' in oct_file_lower:\n",
        "        he_file = oct_file_lower.replace('_real_a', '_real_b')\n",
        "\n",
        "      elif '_fake_a' in oct_file_lower:\n",
        "        he_file = oct_file_lower.replace('_fake_a', '_fake_b')\n",
        "\n",
        "      if he_file in he_files_lower:\n",
        "        he_file = he_files_lower[he_file]\n",
        "        self.paired_files.append((os.path.join(oct_dir, oct_file), os.path.join(he_dir, he_file)))\n",
        "      else:\n",
        "        print(f'Warning: {he_file} not found in {he_dir}')\n",
        "\n",
        "\n",
        "    img_size = 384\n",
        "    self.transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(img_size, interpolation=3, antialias=True),\n",
        "        torchvision.transforms.CenterCrop((img_size, img_size)),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)\n",
        "    ])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paired_files)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    oct_path, he_path = self.paired_files[idx] # retrieve the paired (oct, h&e) file names\n",
        "    oct_image = Image.open(oct_path).convert('RGB')\n",
        "    he_image = Image.open(he_path).convert('RGB')\n",
        "\n",
        "    return self.transform(oct_image), self.transform(he_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "DVtLUoRcimhh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data Loader"
      ],
      "metadata": {
        "id": "L7TuZnlLmyZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create config dictionary\n",
        "train_config = {\n",
        "    \"train_folder\": \"/content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train\",\n",
        "    \"he_val_folder\": \"/content/drive/MyDrive/Teacher_Student_Network/Split Dataset/he_val\",\n",
        "    \"vhe_val_folder\": \"/content/drive/MyDrive/Teacher_Student_Network/Split Dataset/vhe_val\",\n",
        "    \"batch_size\": 128, # Reduced batch size\n",
        "    \"epochs\": 300,\n",
        "    \"shuffle_train\": True,\n",
        "    \"num_workers\": 2,\n",
        "    \"pin_memory\": True,\n",
        "    \"init_lr\": 1e-6,\n",
        "    \"weight_decay\": 0.05,\n",
        "    \"epochs_between_save\": 5,\n",
        "    \"epochs_between_val\": 5,\n",
        "    \"patience\": 5,  # for early stopping\n",
        "    \"output_dir_path\": \"/content/drive/MyDrive/Teacher_Student_Network/model_savepoints\"\n",
        "}\n",
        "\n",
        "train_oct_folder = os.path.join(train_config['train_folder'],'OCT')\n",
        "train_he_folder = os.path.join(train_config['train_folder'],'H&E')\n",
        "\n",
        "val_he_oct_folder = os.path.join(train_config['he_val_folder'], 'OCT')\n",
        "val_he_folder = os.path.join(train_config['he_val_folder'], 'H&E')\n",
        "val_vhe_oct_folder = os.path.join(train_config['vhe_val_folder'], 'OCT')\n",
        "val_vhe_folder = os.path.join(train_config['vhe_val_folder'], 'vH&E')\n",
        "\n",
        "train_dataset = AugmentedImageDataset(train_oct_folder, train_he_folder)\n",
        "he_val_dataset = AugmentedImageDataset(val_he_oct_folder, val_he_folder)\n",
        "vhe_val_dataset = AugmentedImageDataset(val_vhe_oct_folder, val_vhe_folder)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_config['batch_size'],\n",
        "    shuffle=train_config['shuffle_train'],\n",
        "    num_workers=train_config['num_workers'],\n",
        "    pin_memory=train_config['pin_memory']\n",
        ")\n",
        "\n",
        "he_val_loader = DataLoader(\n",
        "    he_val_dataset,\n",
        "    batch_size=train_config['batch_size'],\n",
        "    shuffle=False, # usually false for val\n",
        "    num_workers=train_config['num_workers'],\n",
        "    pin_memory=train_config['pin_memory']\n",
        ")\n",
        "\n",
        "vhe_val_loader = DataLoader(\n",
        "    vhe_val_dataset,\n",
        "    batch_size=train_config['batch_size'],\n",
        "    shuffle=False, # usually false for val\n",
        "    num_workers=train_config['num_workers'],\n",
        "    pin_memory=train_config['pin_memory']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7r7roykm0bG",
        "outputId": "8f688e01-548d-4ff1-deb9-4a42a6d17ef6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: lg-49-slide04_section02_yp0_patch01_augmented_real_b_random_cropped_2 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: lg-09-slide03_section01_yp0_patch01_augmented_real_b_random_cropped_1 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: ld-03-slide01_section02_yp0_patch01_augmented_real_b_uncropped_5 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: ld-12-slide09_section01_yp0_patch01_augmented_real_b_uncropped_2 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: ld-03-slide02_section03_yp0_patch01_augmented_real_b_1 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: ld-06-slide09_section02_yp0_patch01_augmented_real_b_3 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: lg-49-slide04_section03_yp0_patch01_augmented_real_b_4 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: le-09-slide09_section02_yp0_patch01_real_b (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: lg-55-slide04_section01_yp0_patch01_augmented_real_b_1 (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n",
            "Warning: le-09-slide11_section03_yp0_patch01_real_b (1).png not found in /content/drive/MyDrive/Teacher_Student_Network/Split Dataset/train/H&E\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total number of H&E-OCT pairs batches in each epoch in train set: {len(train_loader)}\")\n",
        "print(f\"Total number of H&E-OCT pairs batches in each epoch in val set: {len(he_val_loader)}\")\n",
        "print(f\"Total number of vH&E-OCT pairs batches in each epoch in val set: {len(vhe_val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC5i7U6snhra",
        "outputId": "f77f1e1e-a931-47ec-d710-00d8688158e1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of H&E-OCT pairs batches in each epoch in train set: 45\n",
            "Total number of H&E-OCT pairs batches in each epoch in val set: 5\n",
            "Total number of vH&E-OCT pairs batches in each epoch in val set: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing Loss Function"
      ],
      "metadata": {
        "id": "6iYU0lAv0BX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.nn.functional as F\n",
        "# def cosine_loss(student_emb, teacher_emb):\n",
        "#   return 1 - F.cosine_similarity(student_emb, teacher_emb, dim=1).mean()\n",
        "import torch.nn.functional as F\n",
        "def contrastive_ce(student_emb, teacher_emb, temp = 0.07):\n",
        "  '''\n",
        "  Contrastive cross-entropy loss (InfoNCE-style).\n",
        "  COmpares each student embedding to all teacher embeddings in the batch.\n",
        "  '''\n",
        "\n",
        "  # Normalize embeddings\n",
        "  student_emb = F.normalize(student_emb, dim=-1)\n",
        "  teacher_emb = F.normalize(teacher_emb, dim=-1)\n",
        "\n",
        "  # Compute similarity matrix: (batch_size, batch_size)\n",
        "  logits = torch.matmul(student_emb, teacher_emb.T)\n",
        "  logits /= temp # apply temp scaling\n",
        "\n",
        "  # Ground Truth: i-th student <-> i-th teacher\n",
        "  targets = torch.arange(logits.size(0), device=logits.device) # this tells that H&E correct match of ith OCT is in index i\n",
        "\n",
        "  # Compute cross entropy loss\n",
        "  loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "FvdmgKGQz_w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the saved model checkpoint"
      ],
      "metadata": {
        "id": "sKFPYSXnzxyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(student, teacher, optimizer, config, scheduler=None):\n",
        "  checkpoint_path = os.path.join(config['output_dir_path'], 'larger batch size 128','best_model.pth')\n",
        "\n",
        "  if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    student.load_state_dict(checkpoint['student_state_dict'])\n",
        "\n",
        "    if teacher and checkpoint['teacher_state_dict']:\n",
        "      teacher.load_state_dict(checkpoint['teacher_state_dict'])\n",
        "\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Manually override the learning rate from the checkpoint\n",
        "    for param_group in optimizer.param_groups:\n",
        "      param_group['lr'] = config['init_lr']  # Or directly use 1e-5\n",
        "    print(f\"Learning rate reset to: {optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "\n",
        "    if scheduler and checkpoint['scheduler_state_dict']:\n",
        "      scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    best_val_loss = checkpoint.get('val_loss', float('inf'))\n",
        "    print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "    print(f\"Best val loss: {best_val_loss}\")\n",
        "    return checkpoint['epoch'] + 1, best_val_loss # Resume from next epoch\n",
        "\n",
        "  else:\n",
        "    print(\"No checkpoint found. Starting from scratch.\")\n",
        "    return 0, float('inf')  # Start from epoch 0"
      ],
      "metadata": {
        "id": "B0Kn3CUkz2Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the trained model checkpoint"
      ],
      "metadata": {
        "id": "cqcXiWLuzpr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(student, teacher, optimizer, epoch, config, scheduler=None, val_loss=None, filename='latest_checkpoint.pth'):\n",
        "  print('Saving to: ', config['output_dir_path'])\n",
        "  os.makedirs(config['output_dir_path'], exist_ok=True)\n",
        "\n",
        "  best_model_path = os.path.join(config['output_dir_path'], 'larger batch size 128', 'best_model_2.pth')\n",
        "  torch.save({\n",
        "      'epoch': epoch,\n",
        "      'student_state_dict': student.state_dict(),\n",
        "      'teacher_state_dict': teacher.state_dict() if teacher else None,\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "      'val_loss': val_loss\n",
        "  }, best_model_path)\n",
        "\n",
        "  print(f\"Saved checkpoint at epoch {epoch} with val_loss: {val_loss}\")"
      ],
      "metadata": {
        "id": "zq8rUV7mzt_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "EjyaJGwC0MYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# Filter parameters that require gradients\n",
        "# trainable_params = [p for p in student_model.parameters() if p.requires_grad]\n",
        "optimizer = Adam(student_model.parameters(), lr=train_config['init_lr'], weight_decay=train_config['weight_decay'])\n",
        "scheduler = ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',  # minimize val loss\n",
        "    factor=0.5,  # reduce LR by half\n",
        "    patience=1,  # wait 1 epochs of no improvement\n",
        "    threshold=1e-4, # minimum change to qualify as improvement\n",
        "    verbose=True # print LR changes\n",
        ")\n",
        "print(optimizer.param_groups[0]['lr'])\n",
        "print(optimizer.param_groups[0]['weight_decay'])"
      ],
      "metadata": {
        "id": "yfISm18k0SlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter # for visualizing the loss\n",
        "\n",
        "writer = SummaryWriter(log_dir = 'runs/student_training_run')\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "def train_student(num_epochs, train_loader, he_val_loader, vhe_val_loader, student_model, teacher_model, optimizer, device, scheduler):\n",
        "\n",
        "  scaler = GradScaler()  # Needed for mixed precision\n",
        "\n",
        "  # for early stopping\n",
        "  epochs_without_improvement = 0\n",
        "  patience = train_config['patience']\n",
        "\n",
        "  # Load from checkpoint if available\n",
        "  start_epoch, best_val_loss = load_checkpoint(student_model, teacher_model, optimizer, train_config, scheduler)\n",
        "  # best_val_loss = float('inf')\n",
        "  # start_epoch = 0\n",
        "\n",
        "  ### Training Loop #####\n",
        "  for epoch in range(start_epoch, num_epochs):\n",
        "    student_model.train()  # student in train mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (oct_images, he_images) in enumerate(train_loader):\n",
        "      oct_images = oct_images.to(device)\n",
        "      he_images = he_images.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        teacher_emb = teacher_model(image=he_images, with_head=True, out_norm=True, ms_aug=False, return_global=True)[0]\n",
        "      teacher_emb = teacher_emb.detach()\n",
        "\n",
        "      with autocast(dtype=torch.float16):\n",
        "        student_emb = student_model(image=oct_images, with_head=True, out_norm=True, ms_aug=False, return_global=True)[0]\n",
        "        loss = contrastive_ce(student_emb, teacher_emb)\n",
        "        # print(f\"Student emb mean: {student_emb.mean().item():.4f}, Teacher emb mean: {teacher_emb.mean().item():.4f}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "        if torch.isnan(student_emb).any() or torch.isnan(teacher_emb).any():\n",
        "          print(f\"NaNs detected at batch {batch_idx+1}\")\n",
        "\n",
        "        if student_emb.abs().max() < 1e-6:\n",
        "          print(f\"Student embedding is close to zero at batch {batch_idx+1}\")\n",
        "\n",
        "      optimizer.zero_grad() # zero out gradient\n",
        "      scaler.scale(loss).backward() # backpropagation\n",
        "      scaler.unscale_(optimizer)\n",
        "      torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
        "      scaler.step(optimizer) # update parameters (weight & biases)\n",
        "      scaler.update()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if (batch_idx + 1) % 15 == 0: # print the loss in every 100 batch in each epoch\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}] | Batch [{batch_idx + 1}/{len(train_loader)}] | Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "      # log the loss every 100 batches in each epoch\n",
        "      global_step = epoch * len(train_loader) + batch_idx\n",
        "      writer.add_scalar('Loss/train', loss.item(), global_step)\n",
        "\n",
        "    ### Training Loop ####\n",
        "\n",
        "    ### Printing average loss after each epoch ####\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")\n",
        "    # log average loss at the end of each epoch\n",
        "    writer.add_scalar(\"Loss/epoch_train\", average_loss, epoch)\n",
        "    # log learning rate to tensorboard\n",
        "    writer.add_scalar(\"Learning Rate\", optimizer.param_groups[0]['lr'], epoch)\n",
        "    ### Printing average loss after each epoch ####\n",
        "\n",
        "    torch.cuda.empty_cache()  # Clean memory after each epoch\n",
        "\n",
        "    ### Evaluate every 10 epochs with validation dataset ####\n",
        "    if (epoch + 1) % train_config['epochs_between_val'] == 0:\n",
        "      print(f\"Evaluating at epoch {epoch + 1}\")\n",
        "      student_model.eval() # evaluation mode\n",
        "      he_val_loss_total = 0\n",
        "      vhe_val_loss_total = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for oct_images, he_images in he_val_loader:\n",
        "          oct_images = oct_images.to(device)\n",
        "          he_images = he_images.to(device)\n",
        "\n",
        "          teacher_emb = teacher_model(image=he_images, with_head=True, out_norm=True, ms_aug=False, return_global=True)[0]\n",
        "          student_emb = student_model(image=oct_images, with_head=True, out_norm=True, ms_aug=False, return_global=True)[0]\n",
        "\n",
        "          val_loss = contrastive_ce(student_emb, teacher_emb)\n",
        "          he_val_loss_total += val_loss.item()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for oct_images, vhe_images in vhe_val_loader:\n",
        "          oct_images = oct_images.to(device)\n",
        "          vhe_images = vhe_images.to(device)\n",
        "\n",
        "          teacher_emb = teacher_model(image=vhe_images, with_head=True, out_norm=True, ms_aug=False, return_global=True)[0]\n",
        "          student_emb = student_model(image=oct_images, with_head=True, out_norm=True, ms_aug=False, return_global=True)[0]\n",
        "\n",
        "          val_loss = contrastive_ce(student_emb, teacher_emb)\n",
        "          vhe_val_loss_total += val_loss.item()\n",
        "\n",
        "      avg_he_val_loss = he_val_loss_total / len(he_val_loader)\n",
        "      avg_vhe_val_loss = vhe_val_loss_total / len(vhe_val_loader)\n",
        "\n",
        "      print(f\"Validation Loss (H&E) at Epoch {epoch + 1}: {avg_he_val_loss:.4f}\")\n",
        "      print(f\"Validation Loss (vH&E) at Epoch {epoch + 1}: {avg_vhe_val_loss:.4f}\")\n",
        "\n",
        "      avg_val_loss = (avg_he_val_loss + avg_vhe_val_loss) / 2\n",
        "      print(f\"Average Validation Loss at Epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
        "\n",
        "\n",
        "      scheduler.step(avg_val_loss) # update the learning rate\n",
        "      current_lr = optimizer.param_groups[0]['lr']\n",
        "      print(f\"Learning Rate after scheduler step: {current_lr}\")\n",
        "\n",
        "      torch.cuda.empty_cache()  # Clean memory after each epoch\n",
        "\n",
        "      if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss # update the curr avg val loss if the avg val loss is less than the prev avg val loss\n",
        "        print(f'New best val loss: {best_val_loss}')\n",
        "        epochs_without_improvement = 0\n",
        "        save_checkpoint(student_model, teacher_model, optimizer, epoch, train_config, scheduler, best_val_loss, filename=f\"best_model.pth\")\n",
        "\n",
        "      else:\n",
        "        epochs_without_improvement += 1\n",
        "\n",
        "      if epochs_without_improvement >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch + 1}\") # to prevent overfitting\n",
        "        break\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "FGKEVUeTz51A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_student(\n",
        "    num_epochs=train_config['epochs'],\n",
        "    train_loader=train_loader,\n",
        "    he_val_loader=he_val_loader,\n",
        "    vhe_val_loader=vhe_val_loader,\n",
        "    student_model=histogpt_student,\n",
        "    teacher_model=histogpt_teacher,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    scheduler=scheduler\n",
        "  )"
      ],
      "metadata": {
        "id": "7dtTQF9u0X1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}